{"cells":[{"cell_type":"markdown","metadata":{"id":"oq0eka_aYDQt"},"source":["# First QA Model\n","\n","For our first QA model we will setup a simple question-answering pipeline using HuggingFace transformers and a pretrained BERT model. We will be testing it on our SQuAD data so let's load that first."],"id":"oq0eka_aYDQt"},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pux0ONpWJcru","executionInfo":{"status":"ok","timestamp":1658094758014,"user_tz":-120,"elapsed":11857,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"d079ae56-cda3-4b72-c4d0-34f921e6f0c7"},"id":"pux0ONpWJcru","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 5.0 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 46.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 8.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["import json\n","\n","with open('/content/dev.json', 'r') as f:\n","    squad = json.load(f)"],"metadata":{"id":"x6CDfTeiYTu6","executionInfo":{"status":"ok","timestamp":1658095983224,"user_tz":-120,"elapsed":243,"user":{"displayName":"Na LI","userId":"12754296058413970077"}}},"id":"x6CDfTeiYTu6","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggPWhDBKYDQz"},"source":["As usual, we initialize our transformer tokenizer and model. This time, we will be using a BERT model that has been trained for question-and-answering on the SQuAD dataset. Which is why we will be using the validation dataset (rather than training dataset) from SQuAD."],"id":"ggPWhDBKYDQz"},{"cell_type":"code","execution_count":13,"metadata":{"id":"9AlksandYDQ0","executionInfo":{"status":"ok","timestamp":1658095988138,"user_tz":-120,"elapsed":2767,"user":{"displayName":"Na LI","userId":"12754296058413970077"}}},"outputs":[],"source":["from transformers import BertTokenizer, BertForQuestionAnswering\n","\n","modelname = 'deepset/bert-base-cased-squad2'\n","\n","tokenizer = BertTokenizer.from_pretrained(modelname)\n","model = BertForQuestionAnswering.from_pretrained(modelname)"],"id":"9AlksandYDQ0"},{"cell_type":"markdown","metadata":{"id":"hrDYzT7EYDQ0"},"source":["Transformers comes with a useful class called [`pipeline`](https://huggingface.co/transformers/main_classes/pipelines.html) which allows us to setup easy to use pipelines for common architectures.\n","\n","One of those pipelines is the `question-answering` pipeline which allows us to feed a  dictionary containing a `'question'` and `'context'` and return an answer. Which we initialize like so:"],"id":"hrDYzT7EYDQ0"},{"cell_type":"code","execution_count":14,"metadata":{"id":"Ct_H5qD1YDQ1","executionInfo":{"status":"ok","timestamp":1658095991667,"user_tz":-120,"elapsed":245,"user":{"displayName":"Na LI","userId":"12754296058413970077"}}},"outputs":[],"source":["from transformers import pipeline\n","\n","qa = pipeline('question-answering', model=model, tokenizer=tokenizer)"],"id":"Ct_H5qD1YDQ1"},{"cell_type":"markdown","metadata":{"id":"H6lg2ZCKYDQ2"},"source":["Now we can begin asking questions, let's take a few examples from our `squad` data."],"id":"H6lg2ZCKYDQ2"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BaKdqoJCYDQ3","executionInfo":{"status":"ok","timestamp":1658095994025,"user_tz":-120,"elapsed":252,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"e7e2fc9b-2593-43ca-94c1-7a0194fb9661"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'answer': 'France',\n","  'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n","  'question': 'In what country is Normandy located?'},\n"," {'answer': 'in the 10th and 11th centuries',\n","  'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n","  'question': 'When were the Normans in Normandy?'}]"]},"metadata":{},"execution_count":15}],"source":["squad[:2]"],"id":"BaKdqoJCYDQ3"},{"cell_type":"code","execution_count":16,"metadata":{"id":"iAot9edcYDQ5","executionInfo":{"status":"ok","timestamp":1658096009535,"user_tz":-120,"elapsed":10201,"user":{"displayName":"Na LI","userId":"12754296058413970077"}}},"outputs":[],"source":["# we will intialize a list for answers\n","answers = []\n","\n","for pair in squad[:5]:\n","    # pass in our question and context to return an answer\n","    ans = qa({\n","        'question': pair['question'],\n","        'context': pair['context']\n","    })\n","    # append predicted answer and real to answers list\n","    answers.append({\n","        'predicted': ans['answer'],\n","        'true': pair['answer']\n","    })"],"id":"iAot9edcYDQ5"},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOu20TkEYDQ6","executionInfo":{"status":"ok","timestamp":1658096635421,"user_tz":-120,"elapsed":297,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"aeca5ee2-276f-488d-d4eb-ef0e57b23f62"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'predicted': 'France.', 'true': 'France'},\n"," {'predicted': '10th and 11th centuries',\n","  'true': 'in the 10th and 11th centuries'},\n"," {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n"," {'predicted': 'Denmark, Iceland and Norway',\n","  'true': 'Denmark, Iceland and Norway'},\n"," {'predicted': 'Rollo,', 'true': 'Rollo'}]"]},"metadata":{},"execution_count":17}],"source":["answers"],"id":"lOu20TkEYDQ6"},{"cell_type":"markdown","metadata":{"id":"pdP2GkNQYDQ7"},"source":["So we can see that we're getting almost exact matches. Next, we'll take a look at how we can begin quantifying these results."],"id":"pdP2GkNQYDQ7"},{"cell_type":"markdown","source":["And now we build a list of predicted answers `model_out` and true answers `reference` and calculate the ROUGE score based on these."],"metadata":{"id":"KBANcxcQfoC9"},"id":"KBANcxcQfoC9"},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","model_out = []\n","reference = []\n","\n","for pair in tqdm(squad[0:20], leave=True):\n","    ans = qa({\n","        'question': pair['question'],\n","        'context': pair['context']\n","    })\n","    # append the prediction and reference to the respective lists\n","    model_out.append(ans['answer'])\n","    reference.append(pair['answer'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9Nrz875ePS2","executionInfo":{"status":"ok","timestamp":1658096674813,"user_tz":-120,"elapsed":37280,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"f8025bab-6dea-48df-fa85-0970c3be45a5"},"id":"l9Nrz875ePS2","execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [00:36<00:00,  1.84s/it]\n"]}]},{"cell_type":"code","source":["!pip install rouge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EC1uo0yPQ31l","executionInfo":{"status":"ok","timestamp":1658096696060,"user_tz":-120,"elapsed":5649,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"58a21da1-ffbe-4b43-96bc-8c1f2db166e7"},"id":"EC1uo0yPQ31l","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n"]}]},{"cell_type":"code","source":["from rouge import Rouge\n","\n","# initialize\n","rouge = Rouge()\n","\n","# get scores\n","rouge.get_scores(model_out, reference, avg=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KDXi5XEDey6r","executionInfo":{"status":"ok","timestamp":1658096698353,"user_tz":-120,"elapsed":244,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"caa4ef09-3628-4ed4-c977-15b38d5ae6af"},"id":"KDXi5XEDey6r","execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'rouge-1': {'f': 0.5153968224170924, 'p': 0.5654761904761905, 'r': 0.525},\n"," 'rouge-2': {'f': 0.31249999826562497, 'p': 0.325, 'r': 0.305},\n"," 'rouge-l': {'f': 0.5153968224170924, 'p': 0.5654761904761905, 'r': 0.525}}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["That doesn't seem to be scoring as high as we would expect, if we print some of the results we can see why:"],"metadata":{"id":"lBrIDEyQfXZO"},"id":"lBrIDEyQfXZO"},{"cell_type":"code","source":["# recalculate individual scores\n","scores = rouge.get_scores(model_out, reference)\n","\n","print(model_out[4], ' | ', reference[4], ' | ', scores[4]['rouge-1']['f'])\n","print(model_out[12], ' | ', reference[12], ' | ', scores[12]['rouge-1']['f'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxXWxozRe2qc","executionInfo":{"status":"ok","timestamp":1658096711478,"user_tz":-120,"elapsed":250,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"17bc719b-5bb0-413a-ca44-e68bf070cf71"},"id":"ZxXWxozRe2qc","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Rollo,  |  Rollo  |  0.0\n","William the Conqueror,  |  William the Conqueror  |  0.6666666616666668\n"]}]},{"cell_type":"markdown","source":["Clearly the punctuation differences are causing our ROUGE score to view these words as not matching. To fix this, we'll import `re` and remove any characters that are not spaces, letters, or numbers."],"metadata":{"id":"bThgIKC5fQwj"},"id":"bThgIKC5fQwj"},{"cell_type":"code","source":["import re\n","\n","clean = re.compile('(?i)[^0-9a-z ]')\n","\n","# apply this to both lists\n","model_out = [clean.sub('', text) for text in model_out]\n","reference = [clean.sub('', text) for text in reference]"],"metadata":{"id":"jYxALY-ze_Qo","executionInfo":{"status":"ok","timestamp":1658096729318,"user_tz":-120,"elapsed":437,"user":{"displayName":"Na LI","userId":"12754296058413970077"}}},"id":"jYxALY-ze_Qo","execution_count":23,"outputs":[]},{"cell_type":"code","source":["# recalculate individual scores\n","scores = rouge.get_scores(model_out, reference)\n","\n","print(model_out[4], ' | ', reference[4], ' | ', scores[4]['rouge-1']['f'])\n","print(model_out[12], ' | ', reference[12], ' | ', scores[12]['rouge-1']['f'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d11byclkfCS4","executionInfo":{"status":"ok","timestamp":1658096734236,"user_tz":-120,"elapsed":237,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"62424896-4eec-41c7-ba48-d488382627fd"},"id":"d11byclkfCS4","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Rollo  |  Rollo  |  0.999999995\n","William the Conqueror  |  William the Conqueror  |  0.999999995\n"]}]},{"cell_type":"markdown","source":["These scores are looking better now, let's calculate the average again:"],"metadata":{"id":"LtX9v-FvfJZ-"},"id":"LtX9v-FvfJZ-"},{"cell_type":"code","source":["rouge.get_scores(model_out, reference, avg=True)"],"metadata":{"id":"GPSv_ufyfGUl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658096739273,"user_tz":-120,"elapsed":242,"user":{"displayName":"Na LI","userId":"12754296058413970077"}},"outputId":"cc8a25e7-22aa-41c6-d286-aa234f866098"},"id":"GPSv_ufyfGUl","execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'rouge-1': {'f': 0.5931745999448702,\n","  'p': 0.6392857142857143,\n","  'r': 0.6166666666666666},\n"," 'rouge-2': {'f': 0.34999999815624994, 'p': 0.3571428571428571, 'r': 0.38},\n"," 'rouge-l': {'f': 0.5931745999448702,\n","  'p': 0.6392857142857143,\n","  'r': 0.6166666666666666}}"]},"metadata":{},"execution_count":25}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"03_first_qa_model.ipynb","provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}